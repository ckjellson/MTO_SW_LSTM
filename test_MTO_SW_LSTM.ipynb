{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0           1         2         3         4         5         6     \\\n",
      "0  hej  frameNbr:0  0.503158  0.180337 -0.743345  0.999878  0.518581   \n",
      "1  hej  frameNbr:1  0.500708  0.182518 -0.786058  0.999883  0.517591   \n",
      "2  hej  frameNbr:2  0.498702  0.192451 -0.860069  0.999890  0.516010   \n",
      "3  hej  frameNbr:3  0.495583  0.202635 -0.904024  0.999900  0.513235   \n",
      "4  hej  frameNbr:4  0.494591  0.204605 -0.858029  0.999906  0.510880   \n",
      "\n",
      "       7         8         9     ...      1626      1627      1628      1629  \\\n",
      "0  0.140605 -0.702544  0.999813  ... -0.030246  0.499487  0.846327 -0.034374   \n",
      "1  0.142764 -0.741179  0.999822  ... -0.023965  0.489006  0.844206 -0.028620   \n",
      "2  0.150829 -0.820234  0.999833  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "3  0.157958 -0.861331  0.999848  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "4  0.159458 -0.832953  0.999859  ... -0.019417  0.360299  0.143479 -0.025300   \n",
      "\n",
      "       1630      1631      1632      1633      1634      1635  \n",
      "0  0.518703  0.840132 -0.031596  0.529322  0.832951 -0.028451  \n",
      "1  0.509094  0.840944 -0.027928  0.522820  0.836340 -0.026752  \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4  0.373653  0.127724 -0.025863  0.385764  0.114399 -0.025297  \n",
      "\n",
      "[5 rows x 1636 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/landmarks_403_videos.csv\",encoding = \"ISO-8859-1\", delimiter=\",\", header=None)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data and format for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_idxs = np.argwhere(data.iloc[:,1].to_numpy()=='frameNbr:0')\n",
    "clip_idxs = np.append(clip_idxs,data.shape[0])\n",
    "labels = data.iloc[:,0].to_numpy()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "class_names = label_encoder.classes_\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "np.random.seed(0)\n",
    "for i in range(len(clip_idxs)-1):\n",
    "    rand = np.random.rand()\n",
    "    if rand<0.6:\n",
    "        X_train = torch.from_numpy(data.iloc[clip_idxs[i]:clip_idxs[i+1],2:].to_numpy()).float().to(device)\n",
    "        y_train = torch.from_numpy(y[clip_idxs[i]:clip_idxs[i]+1]).long().to(device)\n",
    "        if X_train.size()[0]==16:\n",
    "            train_data.append((X_train,y_train))\n",
    "    elif rand<0.8:\n",
    "        X_val = torch.from_numpy(data.iloc[clip_idxs[i]:clip_idxs[i+1],2:].to_numpy()).float().to(device)\n",
    "        y_val = torch.from_numpy(y[clip_idxs[i]:clip_idxs[i]+1]).long().to(device)\n",
    "        if X_train.size()[0]==16:\n",
    "            val_data.append((X_val,y_val))\n",
    "    else:\n",
    "        X_test = torch.from_numpy(data.iloc[clip_idxs[i]:clip_idxs[i+1],2:].to_numpy()).float().to(device)\n",
    "        y_test = torch.from_numpy(y[clip_idxs[i]:clip_idxs[i]+1]).long().to(device)\n",
    "        if X_train.size()[0]==16:\n",
    "            test_data.append((X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTO_SW_LSTM(\n",
      "  (rnn): LSTM(3268, 15, batch_first=True, dropout=0.1)\n",
      "  (dnn): Sequential(\n",
      "    (fc0): Linear(in_features=15, out_features=100, bias=True)\n",
      "    (do2): Dropout(p=0.3, inplace=False)\n",
      "    (af2): ReLU()\n",
      "    (lin2): Linear(in_features=100, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckjellson/anaconda3/envs/tecken/lib/python3.8/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "from MTO_SW_LSTM import MTO_SW_LSTM\n",
    "\n",
    "window_size = 2\n",
    "hidden_size = 15\n",
    "num_layers = 1\n",
    "n_features = 1634\n",
    "stride = 1\n",
    "bsize = 20\n",
    "device = 'cpu'\n",
    "bidir = False\n",
    "nout = [100, 4]\n",
    "dropout = 0.1\n",
    "dropout2 = 0.3\n",
    "\n",
    "rnn = MTO_SW_LSTM(window_size,hidden_size,num_layers,n_features,stride,bsize,device,bidir,nout,dropout,dropout2)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.21455860999412835 - Validation loss: 0.43677398562431335\n",
      "Epoch 2 - Training loss: 0.22011500969529152 - Validation loss: 0.4631434381008148\n",
      "Epoch 3 - Training loss: 0.1741049432506164 - Validation loss: 0.2763538956642151\n",
      "Epoch 4 - Training loss: 0.09179465418371062 - Validation loss: 0.2525867521762848\n",
      "Epoch 5 - Training loss: 0.049232218880206347 - Validation loss: 0.2831047773361206\n",
      "Epoch 6 - Training loss: 0.028894211864098907 - Validation loss: 0.2528073489665985\n",
      "Epoch 7 - Training loss: 0.02697721232349674 - Validation loss: 0.27350521087646484\n",
      "Epoch 8 - Training loss: 0.042969910660758615 - Validation loss: 0.17478087544441223\n",
      "Epoch 9 - Training loss: 0.056343377490217485 - Validation loss: 0.20058688521385193\n",
      "Epoch 10 - Training loss: 0.041312293168933444 - Validation loss: 0.259388267993927\n",
      "Epoch 11 - Training loss: 0.04212988393070797 - Validation loss: 0.22310006618499756\n",
      "Epoch 12 - Training loss: 0.017787335440516472 - Validation loss: 0.26084113121032715\n",
      "Epoch 13 - Training loss: 0.019894070690497756 - Validation loss: 0.21836014091968536\n",
      "Epoch 14 - Training loss: 0.019898733939044178 - Validation loss: 0.21490925550460815\n",
      "Epoch 15 - Training loss: 0.017946139773509156 - Validation loss: 0.2206435203552246\n",
      "Epoch 16 - Training loss: 0.024124329405215878 - Validation loss: 0.2627412676811218\n",
      "Epoch 17 - Training loss: 0.018886418799714495 - Validation loss: 0.17894281446933746\n",
      "Epoch 18 - Training loss: 0.029226785292848945 - Validation loss: 0.20974715054035187\n",
      "Epoch 19 - Training loss: 0.01310639048460871 - Validation loss: 0.25181955099105835\n",
      "Epoch 20 - Training loss: 0.0640902064430217 - Validation loss: 0.20422375202178955\n",
      "Epoch 21 - Training loss: 0.060512269070992865 - Validation loss: 0.19937555491924286\n",
      "Epoch 22 - Training loss: 0.036846230660254754 - Validation loss: 0.18390873074531555\n",
      "Epoch 23 - Training loss: 0.042956249128716685 - Validation loss: 0.2237454205751419\n",
      "Epoch 24 - Training loss: 0.025785619955665123 - Validation loss: 0.18996860086917877\n",
      "Epoch 25 - Training loss: 0.025486990227364004 - Validation loss: 0.27061891555786133\n",
      "Epoch 26 - Training loss: 0.01733341731596738 - Validation loss: 0.1898326277732849\n",
      "Epoch 27 - Training loss: 0.03069697793883582 - Validation loss: 0.3677022457122803\n",
      "Epoch 28 - Training loss: 0.04675066711691519 - Validation loss: 0.4340125322341919\n",
      "Epoch 29 - Training loss: 0.08057203702628613 - Validation loss: 0.3411755859851837\n",
      "Epoch 30 - Training loss: 0.04570170771330595 - Validation loss: 0.33143624663352966\n",
      "Epoch 31 - Training loss: 0.03527088618526856 - Validation loss: 0.23751065135002136\n",
      "Epoch 32 - Training loss: 0.01804197490370522 - Validation loss: 0.23113806545734406\n",
      "Epoch 33 - Training loss: 0.015905670026162017 - Validation loss: 0.22556625306606293\n",
      "Epoch 34 - Training loss: 0.01662456541089341 - Validation loss: 0.21974189579486847\n",
      "Epoch 35 - Training loss: 0.017321856944666553 - Validation loss: 0.16882893443107605\n",
      "Epoch 36 - Training loss: 0.009425385505892336 - Validation loss: 0.17939314246177673\n",
      "Epoch 37 - Training loss: 0.03303336751802514 - Validation loss: 0.16566674411296844\n",
      "Epoch 38 - Training loss: 0.08103575809703518 - Validation loss: 0.24779002368450165\n",
      "Epoch 39 - Training loss: 0.022260077375297744 - Validation loss: 0.24830159544944763\n",
      "Epoch 40 - Training loss: 0.008325310055321703 - Validation loss: 0.24522508680820465\n",
      "Epoch 41 - Training loss: 0.018507018277887255 - Validation loss: 0.1808217167854309\n",
      "Epoch 42 - Training loss: 0.029670277416395646 - Validation loss: 0.2625218629837036\n",
      "Epoch 43 - Training loss: 0.010995057818945497 - Validation loss: 0.2217576950788498\n",
      "Epoch 44 - Training loss: 0.016593909822404385 - Validation loss: 0.18218480050563812\n",
      "Epoch 45 - Training loss: 0.007545977345823 - Validation loss: 0.22072669863700867\n",
      "Epoch 46 - Training loss: 0.009477914136368781 - Validation loss: 0.18756070733070374\n",
      "Epoch 47 - Training loss: 0.008121346540671462 - Validation loss: 0.20477928221225739\n",
      "Epoch 48 - Training loss: 0.005824015795951709 - Validation loss: 0.21532970666885376\n",
      "Epoch 49 - Training loss: 0.008241346959645549 - Validation loss: 0.21515895426273346\n",
      "Epoch 50 - Training loss: 0.005928326689172536 - Validation loss: 0.18407657742500305\n",
      "Epoch 51 - Training loss: 0.006278240044290821 - Validation loss: 0.18359938263893127\n",
      "Epoch 52 - Training loss: 0.006636790213330339 - Validation loss: 0.19743439555168152\n",
      "Epoch 53 - Training loss: 0.00968902840395458 - Validation loss: 0.20740699768066406\n",
      "Epoch 54 - Training loss: 0.008817430236376822 - Validation loss: 0.13244225084781647\n",
      "Epoch 55 - Training loss: 0.007319075894580844 - Validation loss: 0.21143175661563873\n",
      "Epoch 56 - Training loss: 0.010762125214872261 - Validation loss: 0.18294164538383484\n",
      "Epoch 57 - Training loss: 0.005898215153138153 - Validation loss: 0.24344971776008606\n",
      "Epoch 58 - Training loss: 0.01896133227273822 - Validation loss: 0.11622416228055954\n",
      "Epoch 59 - Training loss: 0.012841895959960917 - Validation loss: 0.227822944521904\n",
      "Epoch 60 - Training loss: 0.1110158894249859 - Validation loss: 0.45739081501960754\n",
      "Epoch 61 - Training loss: 0.10777522817564507 - Validation loss: 0.3941551148891449\n",
      "Epoch 62 - Training loss: 0.18614598635273674 - Validation loss: 0.3198733329772949\n",
      "Epoch 63 - Training loss: 0.13055766622225443 - Validation loss: 0.15193124115467072\n",
      "Epoch 64 - Training loss: 0.0817406966816634 - Validation loss: 0.2728182375431061\n",
      "Epoch 65 - Training loss: 0.035961444528462984 - Validation loss: 0.26994559168815613\n",
      "Epoch 66 - Training loss: 0.014567505044396967 - Validation loss: 0.28629830479621887\n",
      "Epoch 67 - Training loss: 0.02322066156193614 - Validation loss: 0.19235759973526\n",
      "Epoch 68 - Training loss: 0.011672824000318846 - Validation loss: 0.23001942038536072\n",
      "Epoch 69 - Training loss: 0.009562855111047005 - Validation loss: 0.2481813132762909\n",
      "Epoch 70 - Training loss: 0.01150844645841668 - Validation loss: 0.2113390564918518\n",
      "Epoch 71 - Training loss: 0.007286284138293316 - Validation loss: 0.2279762178659439\n",
      "Epoch 72 - Training loss: 0.006074056349461898 - Validation loss: 0.2221769392490387\n",
      "Epoch 73 - Training loss: 0.004743876876697565 - Validation loss: 0.20835907757282257\n",
      "Epoch 74 - Training loss: 0.004081914395404358 - Validation loss: 0.23047706484794617\n",
      "Epoch 75 - Training loss: 0.006099529205433403 - Validation loss: 0.22342613339424133\n",
      "Epoch 76 - Training loss: 0.004712018669427683 - Validation loss: 0.2137722373008728\n",
      "Epoch 77 - Training loss: 0.006902909163424435 - Validation loss: 0.20105454325675964\n",
      "Epoch 78 - Training loss: 0.00614850810476734 - Validation loss: 0.28280162811279297\n",
      "Epoch 79 - Training loss: 0.010518865727741892 - Validation loss: 0.22719460725784302\n",
      "Epoch 80 - Training loss: 0.004979942241334356 - Validation loss: 0.20345903933048248\n",
      "Epoch 81 - Training loss: 0.0034770834463415667 - Validation loss: 0.1815771460533142\n",
      "Epoch 82 - Training loss: 0.0036727338641261062 - Validation loss: 0.17908160388469696\n",
      "Epoch 83 - Training loss: 0.005526713255676441 - Validation loss: 0.22232389450073242\n",
      "Epoch 84 - Training loss: 0.006117660300030063 - Validation loss: 0.2059234082698822\n",
      "Epoch 85 - Training loss: 0.0071106541048114496 - Validation loss: 0.15164504945278168\n",
      "Epoch 86 - Training loss: 0.004391712757448356 - Validation loss: 0.12316793203353882\n",
      "Epoch 87 - Training loss: 0.005981759624167656 - Validation loss: 0.14905279874801636\n",
      "Epoch 88 - Training loss: 0.011144424303590009 - Validation loss: 0.20745785534381866\n",
      "Epoch 89 - Training loss: 0.016284652995333698 - Validation loss: 0.3412671387195587\n",
      "Epoch 90 - Training loss: 0.019887532379167776 - Validation loss: 0.15292444825172424\n",
      "Epoch 91 - Training loss: 0.02764143410604447 - Validation loss: 0.19385655224323273\n",
      "Epoch 92 - Training loss: 0.12577090431780866 - Validation loss: 0.86653733253479\n",
      "Epoch 93 - Training loss: 0.21933140295247236 - Validation loss: 0.2652454674243927\n",
      "Epoch 94 - Training loss: 0.06520632584579289 - Validation loss: 0.6109654903411865\n",
      "Epoch 95 - Training loss: 0.1351357966195792 - Validation loss: 0.11451183259487152\n",
      "Epoch 96 - Training loss: 0.03736759900736312 - Validation loss: 0.1185523122549057\n",
      "Epoch 97 - Training loss: 0.04980859734738866 - Validation loss: 0.10137917101383209\n",
      "Epoch 98 - Training loss: 0.025925612115922075 - Validation loss: 0.13668091595172882\n",
      "Epoch 99 - Training loss: 0.015224424346039692 - Validation loss: 0.1186264380812645\n",
      "Epoch 100 - Training loss: 0.012716863420791924 - Validation loss: 0.15065348148345947\n",
      "Epoch 101 - Training loss: 0.01759104853651176 - Validation loss: 0.1341196745634079\n",
      "Epoch 102 - Training loss: 0.00866840531428655 - Validation loss: 0.13676290214061737\n",
      "Epoch 103 - Training loss: 0.014269334487228965 - Validation loss: 0.12813909351825714\n",
      "Epoch 104 - Training loss: 0.011278794054912092 - Validation loss: 0.12378980219364166\n",
      "Epoch 105 - Training loss: 0.02021902425137038 - Validation loss: 0.1401529461145401\n",
      "Epoch 106 - Training loss: 0.01008425208662326 - Validation loss: 0.12542688846588135\n",
      "Epoch 107 - Training loss: 0.008211574371671304 - Validation loss: 0.1439386010169983\n",
      "Epoch 108 - Training loss: 0.02248647789626072 - Validation loss: 0.1108497679233551\n",
      "Epoch 109 - Training loss: 0.010047856194432825 - Validation loss: 0.11408357322216034\n",
      "Epoch 110 - Training loss: 0.008625834598205984 - Validation loss: 0.12403376400470734\n",
      "Epoch 111 - Training loss: 0.007120826621151839 - Validation loss: 0.1518465131521225\n",
      "Epoch 112 - Training loss: 0.004841251696537559 - Validation loss: 0.14921793341636658\n",
      "Epoch 113 - Training loss: 0.0075907126447418705 - Validation loss: 0.15523336827754974\n",
      "Epoch 114 - Training loss: 0.008989468158688396 - Validation loss: 0.15372344851493835\n",
      "Epoch 115 - Training loss: 0.004332843100807319 - Validation loss: 0.16264426708221436\n",
      "Epoch 116 - Training loss: 0.00425141652279611 - Validation loss: 0.15871219336986542\n",
      "Epoch 117 - Training loss: 0.004224886278583047 - Validation loss: 0.15792164206504822\n",
      "Epoch 118 - Training loss: 0.006441423814976588 - Validation loss: 0.15215657651424408\n",
      "Epoch 119 - Training loss: 0.004131885909979853 - Validation loss: 0.20418506860733032\n",
      "Epoch 120 - Training loss: 0.00511011338433794 - Validation loss: 0.18943049013614655\n",
      "Epoch 121 - Training loss: 0.004886627963666494 - Validation loss: 0.22116616368293762\n",
      "Epoch 122 - Training loss: 0.005149115953827277 - Validation loss: 0.20162595808506012\n",
      "Epoch 123 - Training loss: 0.0022114742459962144 - Validation loss: 0.20450296998023987\n",
      "Epoch 124 - Training loss: 0.0022709485929226503 - Validation loss: 0.21826772391796112\n",
      "Epoch 125 - Training loss: 0.0025319818620725223 - Validation loss: 0.2305506020784378\n",
      "Epoch 126 - Training loss: 0.0023397848271997645 - Validation loss: 0.23020488023757935\n",
      "Epoch 127 - Training loss: 0.002681474327497805 - Validation loss: 0.2499224841594696\n",
      "Epoch 128 - Training loss: 0.002675705465662759 - Validation loss: 0.20474080741405487\n",
      "Epoch 129 - Training loss: 0.0033616667496971786 - Validation loss: 0.20352648198604584\n",
      "Epoch 130 - Training loss: 0.0031740457391909636 - Validation loss: 0.17837414145469666\n",
      "Epoch 131 - Training loss: 0.007384513315628283 - Validation loss: 0.2254875749349594\n",
      "Epoch 132 - Training loss: 0.0037013994976102063 - Validation loss: 0.318146675825119\n",
      "Epoch 133 - Training loss: 0.0025096933920091638 - Validation loss: 0.25129491090774536\n",
      "Epoch 134 - Training loss: 0.0036657175029783198 - Validation loss: 0.21032962203025818\n",
      "Epoch 135 - Training loss: 0.002445897819901196 - Validation loss: 0.18024803698062897\n",
      "Epoch 136 - Training loss: 0.0020664853509515524 - Validation loss: 0.24683307111263275\n",
      "Epoch 137 - Training loss: 0.001629355732196321 - Validation loss: 0.2034378945827484\n",
      "Epoch 138 - Training loss: 0.0017147902035503648 - Validation loss: 0.1868176907300949\n",
      "Epoch 139 - Training loss: 0.0018253064408781938 - Validation loss: 0.17534634470939636\n",
      "Epoch 140 - Training loss: 0.0023130414653375433 - Validation loss: 0.20143359899520874\n",
      "Epoch 141 - Training loss: 0.0018262037701788358 - Validation loss: 0.1861812025308609\n",
      "Epoch 142 - Training loss: 0.003846209750918206 - Validation loss: 0.2209341675043106\n",
      "Epoch 143 - Training loss: 0.0023161877931367294 - Validation loss: 0.23375150561332703\n",
      "Epoch 144 - Training loss: 0.012364859489025548 - Validation loss: 0.08686871081590652\n",
      "Epoch 145 - Training loss: 0.0534352153869501 - Validation loss: 0.36560824513435364\n",
      "Epoch 146 - Training loss: 0.1088598466400678 - Validation loss: 0.3680966794490814\n",
      "Epoch 147 - Training loss: 0.09170438860504267 - Validation loss: 0.15799468755722046\n",
      "Epoch 148 - Training loss: 0.09276912966743112 - Validation loss: 0.4882599115371704\n",
      "Epoch 149 - Training loss: 0.17634731985162944 - Validation loss: 0.3183923065662384\n",
      "Epoch 150 - Training loss: 0.09081956193161507 - Validation loss: 0.23755989968776703\n",
      "Epoch 151 - Training loss: 0.04158336014370434 - Validation loss: 0.21778780221939087\n",
      "Epoch 152 - Training loss: 0.023810040729586035 - Validation loss: 0.24954025447368622\n",
      "Epoch 153 - Training loss: 0.0665911542212901 - Validation loss: 0.22022786736488342\n",
      "Epoch 154 - Training loss: 0.050764597641925015 - Validation loss: 0.1965746283531189\n",
      "Epoch 155 - Training loss: 0.01909209131069171 - Validation loss: 0.13868486881256104\n",
      "Epoch 156 - Training loss: 0.0038174867319564023 - Validation loss: 0.14489321410655975\n",
      "Epoch 157 - Training loss: 0.004924096948040339 - Validation loss: 0.1517450362443924\n",
      "Epoch 158 - Training loss: 0.00466634063438202 - Validation loss: 0.15428411960601807\n",
      "Epoch 159 - Training loss: 0.00238944032995884 - Validation loss: 0.15793433785438538\n",
      "Epoch 160 - Training loss: 0.002815469565878933 - Validation loss: 0.16101893782615662\n",
      "Epoch 161 - Training loss: 0.0020125915616517887 - Validation loss: 0.16468466818332672\n",
      "Epoch 162 - Training loss: 0.004520286844732861 - Validation loss: 0.1646534949541092\n",
      "Epoch 163 - Training loss: 0.0035147044060674184 - Validation loss: 0.16691842675209045\n",
      "Epoch 164 - Training loss: 0.0015256052526334922 - Validation loss: 0.1659974902868271\n",
      "Epoch 165 - Training loss: 0.0038136634490607926 - Validation loss: 0.16685113310813904\n",
      "Epoch 166 - Training loss: 0.0036063972608341524 - Validation loss: 0.1720760315656662\n",
      "Epoch 167 - Training loss: 0.002296047801792156 - Validation loss: 0.1616654396057129\n",
      "Epoch 168 - Training loss: 0.002643944423956176 - Validation loss: 0.18197405338287354\n",
      "Epoch 169 - Training loss: 0.0015704159256226073 - Validation loss: 0.17526407539844513\n",
      "Epoch 170 - Training loss: 0.0014653725423462067 - Validation loss: 0.16998417675495148\n",
      "Epoch 171 - Training loss: 0.001928289158968255 - Validation loss: 0.18056704103946686\n",
      "Epoch 172 - Training loss: 0.0015713840984972194 - Validation loss: 0.17072872817516327\n",
      "Epoch 173 - Training loss: 0.001658394326417086 - Validation loss: 0.18486429750919342\n",
      "Epoch 174 - Training loss: 0.0017241558574217681 - Validation loss: 0.1664758175611496\n",
      "Epoch 175 - Training loss: 0.0019067556446922633 - Validation loss: 0.16706721484661102\n",
      "Epoch 176 - Training loss: 0.0016586418908749085 - Validation loss: 0.17094439268112183\n",
      "Epoch 177 - Training loss: 0.001701367794642768 - Validation loss: 0.18676364421844482\n",
      "Epoch 178 - Training loss: 0.0014995036229568843 - Validation loss: 0.18138164281845093\n",
      "Epoch 179 - Training loss: 0.0013737601645213242 - Validation loss: 0.17222799360752106\n",
      "Epoch 180 - Training loss: 0.0014951307869826753 - Validation loss: 0.18774697184562683\n",
      "Epoch 181 - Training loss: 0.0011098701919157368 - Validation loss: 0.18248975276947021\n",
      "Epoch 182 - Training loss: 0.002552373698563315 - Validation loss: 0.19838856160640717\n",
      "Epoch 183 - Training loss: 0.0015689353070532281 - Validation loss: 0.169187530875206\n",
      "Epoch 184 - Training loss: 0.0009512542164884508 - Validation loss: 0.16261766850948334\n",
      "Epoch 185 - Training loss: 0.0031003060648799874 - Validation loss: 0.20321893692016602\n",
      "Epoch 186 - Training loss: 0.0011494090261597496 - Validation loss: 0.11893025040626526\n",
      "Epoch 187 - Training loss: 0.002486175112911345 - Validation loss: 0.22287936508655548\n",
      "Epoch 188 - Training loss: 0.0053138764124014415 - Validation loss: 0.3118385374546051\n",
      "Epoch 189 - Training loss: 0.007976365314486126 - Validation loss: 0.3412378132343292\n",
      "Epoch 190 - Training loss: 0.007689720693936882 - Validation loss: 0.39437538385391235\n",
      "Epoch 191 - Training loss: 0.011427501953827838 - Validation loss: 0.327072411775589\n",
      "Epoch 192 - Training loss: 0.1677261149355521 - Validation loss: 0.3503156006336212\n",
      "Epoch 193 - Training loss: 0.1124162661532561 - Validation loss: 0.20199942588806152\n",
      "Epoch 194 - Training loss: 0.056425185583066195 - Validation loss: 0.294447124004364\n",
      "Epoch 195 - Training loss: 0.028120912291342393 - Validation loss: 0.2645324766635895\n",
      "Epoch 196 - Training loss: 0.026941056770738214 - Validation loss: 0.22952772676944733\n",
      "Epoch 197 - Training loss: 0.009531596335970486 - Validation loss: 0.211325541138649\n",
      "Epoch 198 - Training loss: 0.007046938170484888 - Validation loss: 0.19132904708385468\n",
      "Epoch 199 - Training loss: 0.005751164911392455 - Validation loss: 0.21457606554031372\n",
      "Epoch 200 - Training loss: 0.009373005348606966 - Validation loss: 0.2046625167131424\n",
      "Epoch 201 - Training loss: 0.01081867717342296 - Validation loss: 0.2241905778646469\n",
      "Epoch 202 - Training loss: 0.006505632161861286 - Validation loss: 0.24014616012573242\n",
      "Epoch 203 - Training loss: 0.008484627629513852 - Validation loss: 0.23949088156223297\n",
      "Epoch 204 - Training loss: 0.010579842928564176 - Validation loss: 0.23218882083892822\n",
      "Epoch 205 - Training loss: 0.007505381111210833 - Validation loss: 0.24994707107543945\n",
      "Epoch 206 - Training loss: 0.006686017535685096 - Validation loss: 0.27227136492729187\n",
      "Epoch 207 - Training loss: 0.006396905424480792 - Validation loss: 0.2640664279460907\n",
      "Epoch 208 - Training loss: 0.020908727892674506 - Validation loss: 0.28685706853866577\n",
      "Epoch 209 - Training loss: 0.08156282677858447 - Validation loss: 0.39758092164993286\n",
      "Epoch 210 - Training loss: 0.04198606469435617 - Validation loss: 0.38715988397598267\n",
      "Epoch 211 - Training loss: 0.0668814355158247 - Validation loss: 0.3336943984031677\n",
      "Epoch 212 - Training loss: 0.07934699154187304 - Validation loss: 0.2658192217350006\n",
      "Epoch 213 - Training loss: 0.08461376725851248 - Validation loss: 0.28333237767219543\n",
      "Epoch 214 - Training loss: 0.04146730219751286 - Validation loss: 0.47182220220565796\n",
      "Epoch 215 - Training loss: 0.04946656556179126 - Validation loss: 0.22383324801921844\n",
      "Epoch 216 - Training loss: 0.024766650846383225 - Validation loss: 0.1473797857761383\n",
      "Epoch 217 - Training loss: 0.014257146627642214 - Validation loss: 0.159019336104393\n",
      "Epoch 218 - Training loss: 0.021154082225014765 - Validation loss: 0.13234736025333405\n",
      "Epoch 219 - Training loss: 0.013185441818980811 - Validation loss: 0.11776162683963776\n",
      "Epoch 220 - Training loss: 0.004709940258180723 - Validation loss: 0.13642293214797974\n",
      "Epoch 221 - Training loss: 0.004711154033429921 - Validation loss: 0.12524354457855225\n",
      "Epoch 222 - Training loss: 0.008208107896886455 - Validation loss: 0.12435365468263626\n",
      "Epoch 223 - Training loss: 0.004670318196682881 - Validation loss: 0.12526477873325348\n",
      "Epoch 224 - Training loss: 0.005051802999029557 - Validation loss: 0.15592457354068756\n",
      "Epoch 225 - Training loss: 0.006574880351157238 - Validation loss: 0.1386147439479828\n",
      "Epoch 226 - Training loss: 0.005989155198525016 - Validation loss: 0.14608176052570343\n",
      "Epoch 227 - Training loss: 0.006923181877937168 - Validation loss: 0.13168013095855713\n",
      "Epoch 228 - Training loss: 0.004440417178557254 - Validation loss: 0.1274026781320572\n",
      "Epoch 229 - Training loss: 0.030761558999074623 - Validation loss: 0.26889610290527344\n",
      "Epoch 230 - Training loss: 0.0235623205662705 - Validation loss: 0.1937083750963211\n",
      "Epoch 231 - Training loss: 0.04656996461562812 - Validation loss: 0.2709014117717743\n",
      "Epoch 232 - Training loss: 0.23313082564466944 - Validation loss: 0.6190059781074524\n",
      "Epoch 233 - Training loss: 0.08879445019798975 - Validation loss: 0.1755044013261795\n",
      "Epoch 234 - Training loss: 0.03233724054977453 - Validation loss: 0.18084967136383057\n",
      "Epoch 235 - Training loss: 0.008722519773679474 - Validation loss: 0.16197313368320465\n",
      "Epoch 236 - Training loss: 0.006704353732250941 - Validation loss: 0.14544455707073212\n",
      "Epoch 237 - Training loss: 0.009652154665673152 - Validation loss: 0.1273898184299469\n",
      "Epoch 238 - Training loss: 0.0034638737803713107 - Validation loss: 0.13545775413513184\n",
      "Epoch 239 - Training loss: 0.0037675416921653473 - Validation loss: 0.12802547216415405\n",
      "Epoch 240 - Training loss: 0.0033837187656899914 - Validation loss: 0.14901286363601685\n",
      "Epoch 241 - Training loss: 0.003461711467631782 - Validation loss: 0.13577622175216675\n",
      "Epoch 242 - Training loss: 0.004294020385714248 - Validation loss: 0.13239257037639618\n",
      "Epoch 243 - Training loss: 0.0032980465572715425 - Validation loss: 0.13475348055362701\n",
      "Epoch 244 - Training loss: 0.002768702072595867 - Validation loss: 0.14013029634952545\n",
      "Epoch 245 - Training loss: 0.001959465788483309 - Validation loss: 0.1427038162946701\n",
      "Epoch 246 - Training loss: 0.0030305447920303172 - Validation loss: 0.14282698929309845\n",
      "Epoch 247 - Training loss: 0.00658528216687652 - Validation loss: 0.1497175097465515\n",
      "Epoch 248 - Training loss: 0.002939943786865721 - Validation loss: 0.15619254112243652\n",
      "Epoch 249 - Training loss: 0.0021828320459462702 - Validation loss: 0.15691427886486053\n",
      "Epoch 250 - Training loss: 0.0035260184619498127 - Validation loss: 0.14599841833114624\n",
      "Epoch 251 - Training loss: 0.0029046401299031763 - Validation loss: 0.17756201326847076\n",
      "Epoch 252 - Training loss: 0.003088229898518572 - Validation loss: 0.1686462014913559\n",
      "Epoch 253 - Training loss: 0.004493758358876221 - Validation loss: 0.1343117654323578\n",
      "Epoch 254 - Training loss: 0.004239747104293201 - Validation loss: 0.15728972852230072\n",
      "Epoch 255 - Training loss: 0.0038001408296016357 - Validation loss: 0.1274665743112564\n",
      "Epoch 256 - Training loss: 0.003325635062841078 - Validation loss: 0.17910023033618927\n",
      "Epoch 257 - Training loss: 0.0018151986732846126 - Validation loss: 0.16994445025920868\n",
      "Epoch 258 - Training loss: 0.0016403291519964114 - Validation loss: 0.14625254273414612\n",
      "Epoch 259 - Training loss: 0.0017903744398305814 - Validation loss: 0.158408060669899\n",
      "Epoch 260 - Training loss: 0.0012202973157400265 - Validation loss: 0.16021066904067993\n",
      "Epoch 261 - Training loss: 0.004040200243859242 - Validation loss: 0.17881418764591217\n",
      "Epoch 262 - Training loss: 0.002540788004504672 - Validation loss: 0.1720706969499588\n",
      "Epoch 263 - Training loss: 0.0015451025865331758 - Validation loss: 0.21204838156700134\n",
      "Epoch 264 - Training loss: 0.0034780399461548464 - Validation loss: 0.17387917637825012\n",
      "Epoch 265 - Training loss: 0.003961327815583597 - Validation loss: 0.18385085463523865\n",
      "Epoch 266 - Training loss: 0.001489843428619982 - Validation loss: 0.2103685438632965\n",
      "Epoch 267 - Training loss: 0.0018745562750458096 - Validation loss: 0.18636398017406464\n",
      "Epoch 268 - Training loss: 0.0025304606970166788 - Validation loss: 0.2166251242160797\n",
      "Epoch 269 - Training loss: 0.0012665035137615632 - Validation loss: 0.19190369546413422\n",
      "Epoch 270 - Training loss: 0.0011118271819820318 - Validation loss: 0.17466220259666443\n",
      "Epoch 271 - Training loss: 0.002113185120833805 - Validation loss: 0.19263537228107452\n",
      "Epoch 272 - Training loss: 0.0013857169700107381 - Validation loss: 0.18809090554714203\n",
      "Epoch 273 - Training loss: 0.0013679871359878841 - Validation loss: 0.2048453539609909\n",
      "Epoch 274 - Training loss: 0.0014666488835549292 - Validation loss: 0.19711188971996307\n",
      "Epoch 275 - Training loss: 0.009076242085332828 - Validation loss: 0.21435056626796722\n",
      "Epoch 276 - Training loss: 0.002686863375856774 - Validation loss: 0.23873382806777954\n",
      "Epoch 277 - Training loss: 0.0020354895556617216 - Validation loss: 0.20172671973705292\n",
      "Epoch 278 - Training loss: 0.0017324641788339552 - Validation loss: 0.22495457530021667\n",
      "Epoch 279 - Training loss: 0.004452158853382571 - Validation loss: 0.1519731730222702\n",
      "Epoch 280 - Training loss: 0.024656486304593273 - Validation loss: 0.503228485584259\n",
      "Epoch 281 - Training loss: 0.12572507604879016 - Validation loss: 0.1890731304883957\n",
      "Epoch 282 - Training loss: 0.13822497660294175 - Validation loss: 0.27045994997024536\n",
      "Epoch 283 - Training loss: 0.16919406553885588 - Validation loss: 0.4521925747394562\n",
      "Epoch 284 - Training loss: 0.10880457625413935 - Validation loss: 0.16145621240139008\n",
      "Epoch 285 - Training loss: 0.021035148257700104 - Validation loss: 0.307674765586853\n",
      "Epoch 286 - Training loss: 0.032546703422364466 - Validation loss: 0.1472262293100357\n",
      "Epoch 287 - Training loss: 0.02232159659130654 - Validation loss: 0.19996857643127441\n",
      "Epoch 288 - Training loss: 0.01339890762271049 - Validation loss: 0.14992298185825348\n",
      "Epoch 289 - Training loss: 0.004001545089219387 - Validation loss: 0.16032595932483673\n",
      "Epoch 290 - Training loss: 0.002414416492683813 - Validation loss: 0.15986719727516174\n",
      "Epoch 291 - Training loss: 0.0033851389550060653 - Validation loss: 0.15797573328018188\n",
      "Epoch 292 - Training loss: 0.002325844257332695 - Validation loss: 0.16057690978050232\n",
      "Epoch 293 - Training loss: 0.005025483786691136 - Validation loss: 0.17969535291194916\n",
      "Epoch 294 - Training loss: 0.004264294356592775 - Validation loss: 0.16234247386455536\n",
      "Epoch 295 - Training loss: 0.0025867927809789157 - Validation loss: 0.15752893686294556\n",
      "Epoch 296 - Training loss: 0.0018705806408737165 - Validation loss: 0.16355788707733154\n",
      "Epoch 297 - Training loss: 0.00206511230499018 - Validation loss: 0.16866804659366608\n",
      "Epoch 298 - Training loss: 0.002131559029900624 - Validation loss: 0.16519953310489655\n",
      "Epoch 299 - Training loss: 0.0026938043908254863 - Validation loss: 0.19661226868629456\n",
      "Epoch 300 - Training loss: 0.003059722570469603 - Validation loss: 0.16234740614891052\n",
      "Model saved at epoch:  144\n",
      "Finished training in  45.679248094558716  seconds\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "learning_rate = 0.0001\n",
    "using_gpu = False\n",
    "testnbr = 1\n",
    "rnn.train_model(train_data, val_data, epochs, bsize, learning_rate, using_gpu, testnbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, out, y_test = rnn.test_model(test_data,using_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute and print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         hej       0.93      0.87      0.90        15\n",
      "       kaffe       1.00      1.00      1.00        20\n",
      "       mjÃ¶lk       0.96      1.00      0.98        24\n",
      "        tack       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.96        74\n",
      "   macro avg       0.96      0.95      0.95        74\n",
      "weighted avg       0.96      0.96      0.96        74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = torch.argmax(probs,dim=1).numpy()\n",
    "truth = np.array([y.numpy() for y in y_test])\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(truth,preds,target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test simple Classifier Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(train_data),1634*16))\n",
    "y_train = np.zeros((len(train_data),))\n",
    "for i in range(len(train_data)):\n",
    "    X_train[i,:] = train_data[i][0].numpy().flatten()\n",
    "    y_train[i] = train_data[i][1].numpy()\n",
    "X_test = np.zeros((len(test_data),1634*16))\n",
    "y_test = np.zeros((len(test_data),))\n",
    "for i in range(len(test_data)):\n",
    "    X_test[i,:] = test_data[i][0].numpy().flatten()\n",
    "    y_test[i] = test_data[i][1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9864864864864865\n"
     ]
    }
   ],
   "source": [
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d14b8d03698bf490b6ff48dbb8e04bb775f0bd56b42be1a84e036e4447c0d02"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tecken': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
