{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0           1         2         3         4         5         6     \\\n",
      "0  hej  frameNbr:0  0.503158  0.180337 -0.743345  0.999878  0.518581   \n",
      "1  hej  frameNbr:1  0.500708  0.182518 -0.786058  0.999883  0.517591   \n",
      "2  hej  frameNbr:2  0.498702  0.192451 -0.860069  0.999890  0.516010   \n",
      "3  hej  frameNbr:3  0.495583  0.202635 -0.904024  0.999900  0.513235   \n",
      "4  hej  frameNbr:4  0.494591  0.204605 -0.858029  0.999906  0.510880   \n",
      "\n",
      "       7         8         9     ...      1626      1627      1628      1629  \\\n",
      "0  0.140605 -0.702544  0.999813  ... -0.030246  0.499487  0.846327 -0.034374   \n",
      "1  0.142764 -0.741179  0.999822  ... -0.023965  0.489006  0.844206 -0.028620   \n",
      "2  0.150829 -0.820234  0.999833  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "3  0.157958 -0.861331  0.999848  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "4  0.159458 -0.832953  0.999859  ... -0.019417  0.360299  0.143479 -0.025300   \n",
      "\n",
      "       1630      1631      1632      1633      1634      1635  \n",
      "0  0.518703  0.840132 -0.031596  0.529322  0.832951 -0.028451  \n",
      "1  0.509094  0.840944 -0.027928  0.522820  0.836340 -0.026752  \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4  0.373653  0.127724 -0.025863  0.385764  0.114399 -0.025297  \n",
      "\n",
      "[5 rows x 1636 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"landmarks.csv\",encoding = \"ISO-8859-1\", delimiter=\",\", header=None)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data and format for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_idxs = np.argwhere(data.iloc[:,1].to_numpy()=='frameNbr:0')\n",
    "clip_idxs = np.append(clip_idxs,data.shape[0])\n",
    "labels = data.iloc[:,0].to_numpy()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "class_names = label_encoder.classes_\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "np.random.seed(0)\n",
    "for i in range(len(clip_idxs)-1):\n",
    "    rand = np.random.rand()\n",
    "    if rand<0.6:\n",
    "        X_train = torch.from_numpy(data.iloc[clip_idxs[i]:clip_idxs[i+1],2:].to_numpy()).float().to(device)\n",
    "        y_train = torch.from_numpy(y[clip_idxs[i]:clip_idxs[i]+1]).long().to(device)\n",
    "        train_data.append((X_train,y_train))\n",
    "    elif rand<0.8:\n",
    "        X_val = torch.from_numpy(data.iloc[clip_idxs[i]:clip_idxs[i+1],2:].to_numpy()).float().to(device)\n",
    "        y_val = torch.from_numpy(y[clip_idxs[i]:clip_idxs[i]+1]).long().to(device)\n",
    "        val_data.append((X_val,y_val))\n",
    "    else:\n",
    "        X_test = torch.from_numpy(data.iloc[clip_idxs[i]:clip_idxs[i+1],2:].to_numpy()).float().to(device)\n",
    "        y_test = torch.from_numpy(y[clip_idxs[i]:clip_idxs[i]+1]).long().to(device)\n",
    "        test_data.append((X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTO_SW_LSTM(\n",
      "  (rnn): LSTM(3268, 10, batch_first=True, dropout=0.1)\n",
      "  (dnn): Sequential(\n",
      "    (fc0): Linear(in_features=10, out_features=100, bias=True)\n",
      "    (do2): Dropout(p=0.2, inplace=False)\n",
      "    (af2): Tanh()\n",
      "    (lin2): Linear(in_features=100, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckjellson/anaconda3/envs/tecken/lib/python3.8/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "from MTO_SW_LSTM import MTO_SW_LSTM\n",
    "\n",
    "window_size = 2\n",
    "hidden_size = 10\n",
    "num_layers = 1\n",
    "n_features = 1634\n",
    "stride = 1\n",
    "bsize = 10\n",
    "device = 'cpu'\n",
    "bidir = False\n",
    "nout = [100, 4]\n",
    "dropout = 0.1\n",
    "dropout2 = 0.2\n",
    "\n",
    "rnn = MTO_SW_LSTM(window_size,hidden_size,num_layers,n_features,stride,bsize,device,bidir,nout,dropout,dropout2)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 1.3964620729287465 - Validation loss: 1.3564848899841309\n",
      "Epoch 2 - Training loss: 1.3396759529908497 - Validation loss: 1.3235909938812256\n",
      "Epoch 3 - Training loss: 1.3161173164844513 - Validation loss: 1.3149713277816772\n",
      "Epoch 4 - Training loss: 1.306538258989652 - Validation loss: 1.2853120565414429\n",
      "Epoch 5 - Training loss: 1.2851731479167938 - Validation loss: 1.2676494121551514\n",
      "Epoch 6 - Training loss: 1.2626171658436458 - Validation loss: 1.2796893119812012\n",
      "Epoch 7 - Training loss: 1.2722961902618408 - Validation loss: 1.2518397569656372\n",
      "Epoch 8 - Training loss: 1.237175037463506 - Validation loss: 1.2337125539779663\n",
      "Epoch 9 - Training loss: 1.2259008536736171 - Validation loss: 1.2268424034118652\n",
      "Epoch 10 - Training loss: 1.2112764517466228 - Validation loss: 1.2152807712554932\n",
      "Epoch 11 - Training loss: 1.1897220636407535 - Validation loss: 1.2154200077056885\n",
      "Epoch 12 - Training loss: 1.1881271998087566 - Validation loss: 1.1882588863372803\n",
      "Epoch 13 - Training loss: 1.1592150206367176 - Validation loss: 1.183379054069519\n",
      "Epoch 14 - Training loss: 1.1683053622643154 - Validation loss: 1.1638213396072388\n",
      "Epoch 15 - Training loss: 1.1651944393912952 - Validation loss: 1.1827360391616821\n",
      "Epoch 16 - Training loss: 1.131750186284383 - Validation loss: 1.2773098945617676\n",
      "Epoch 17 - Training loss: 1.1731836174925168 - Validation loss: 1.1688661575317383\n",
      "Epoch 18 - Training loss: 1.1636280491948128 - Validation loss: 1.1856848001480103\n",
      "Epoch 19 - Training loss: 1.123901645342509 - Validation loss: 1.1431089639663696\n",
      "Epoch 20 - Training loss: 1.104488675793012 - Validation loss: 1.169567346572876\n",
      "Epoch 21 - Training loss: 1.110427665213744 - Validation loss: 1.13970148563385\n",
      "Epoch 22 - Training loss: 1.108697695036729 - Validation loss: 1.1406431198120117\n",
      "Epoch 23 - Training loss: 1.0697940265138943 - Validation loss: 1.1074472665786743\n",
      "Epoch 24 - Training loss: 1.05233783274889 - Validation loss: 1.1259125471115112\n",
      "Epoch 25 - Training loss: 1.0580573504169781 - Validation loss: 1.1206846237182617\n",
      "Epoch 26 - Training loss: 1.06063262373209 - Validation loss: 1.156378149986267\n",
      "Epoch 27 - Training loss: 1.0439694697658222 - Validation loss: 1.0814980268478394\n",
      "Epoch 28 - Training loss: 1.0464916775623958 - Validation loss: 1.0859836339950562\n",
      "Epoch 29 - Training loss: 1.0273542379339535 - Validation loss: 1.111324667930603\n",
      "Epoch 30 - Training loss: 1.006426642338435 - Validation loss: 1.085893154144287\n",
      "Epoch 31 - Training loss: 1.047390525539716 - Validation loss: 1.0566439628601074\n",
      "Epoch 32 - Training loss: 0.9802145212888718 - Validation loss: 1.0811420679092407\n",
      "Epoch 33 - Training loss: 0.977565253774325 - Validation loss: 1.0365575551986694\n",
      "Epoch 34 - Training loss: 0.9652371307214102 - Validation loss: 1.0763736963272095\n",
      "Epoch 35 - Training loss: 0.9703411236405373 - Validation loss: 1.0099866390228271\n",
      "Epoch 36 - Training loss: 0.9442285448312759 - Validation loss: 1.0529778003692627\n",
      "Epoch 37 - Training loss: 0.9221391454339027 - Validation loss: 1.0190540552139282\n",
      "Epoch 38 - Training loss: 0.9173441231250763 - Validation loss: 1.0037031173706055\n",
      "Epoch 39 - Training loss: 0.8838701422015826 - Validation loss: 0.9593420624732971\n",
      "Epoch 40 - Training loss: 0.9076297755042712 - Validation loss: 0.9708129167556763\n",
      "Epoch 41 - Training loss: 0.8697332764665285 - Validation loss: 0.9309024810791016\n",
      "Epoch 42 - Training loss: 0.8491466815272967 - Validation loss: 0.9271228313446045\n",
      "Epoch 43 - Training loss: 0.8388375913103422 - Validation loss: 0.918350100517273\n",
      "Epoch 44 - Training loss: 0.8426968082785606 - Validation loss: 0.9871518015861511\n",
      "Epoch 45 - Training loss: 0.8528650601704916 - Validation loss: 1.0031986236572266\n",
      "Epoch 46 - Training loss: 0.8405896524588267 - Validation loss: 0.9306108355522156\n",
      "Epoch 47 - Training loss: 0.8072839627663294 - Validation loss: 0.8666340708732605\n",
      "Epoch 48 - Training loss: 0.7993694196144739 - Validation loss: 0.893480122089386\n",
      "Epoch 49 - Training loss: 0.7864850709835688 - Validation loss: 0.8834546208381653\n",
      "Epoch 50 - Training loss: 0.767576277256012 - Validation loss: 0.8187810778617859\n",
      "Epoch 51 - Training loss: 0.7203313137094179 - Validation loss: 0.8442119359970093\n",
      "Epoch 52 - Training loss: 0.7559282059470812 - Validation loss: 0.9252704381942749\n",
      "Epoch 53 - Training loss: 0.7849714159965515 - Validation loss: 0.9118068218231201\n",
      "Epoch 54 - Training loss: 0.71096404393514 - Validation loss: 0.7948764562606812\n",
      "Epoch 55 - Training loss: 0.7066493431727091 - Validation loss: 0.7898509502410889\n",
      "Epoch 56 - Training loss: 0.7062662641207377 - Validation loss: 0.8100851774215698\n",
      "Epoch 57 - Training loss: 0.674135667582353 - Validation loss: 0.7466588020324707\n",
      "Epoch 58 - Training loss: 0.6827229025463263 - Validation loss: 0.7220032215118408\n",
      "Epoch 59 - Training loss: 0.6623548455536366 - Validation loss: 0.7615528702735901\n",
      "Epoch 60 - Training loss: 0.6344069118301073 - Validation loss: 0.7324113249778748\n",
      "Epoch 61 - Training loss: 0.6054636252423128 - Validation loss: 0.7005837559700012\n",
      "Epoch 62 - Training loss: 0.6348201433817545 - Validation loss: 0.7324467301368713\n",
      "Epoch 63 - Training loss: 0.5703680999577045 - Validation loss: 0.6940270662307739\n",
      "Epoch 64 - Training loss: 0.5601592548191547 - Validation loss: 0.6762472987174988\n",
      "Epoch 65 - Training loss: 0.5582706729571024 - Validation loss: 0.6605566143989563\n",
      "Epoch 66 - Training loss: 0.5436777410407861 - Validation loss: 0.6256188750267029\n",
      "Epoch 67 - Training loss: 0.5072818510234356 - Validation loss: 0.6630336046218872\n",
      "Epoch 68 - Training loss: 0.4945472528537114 - Validation loss: 0.5931439399719238\n",
      "Epoch 69 - Training loss: 0.47833053519328433 - Validation loss: 0.5861152410507202\n",
      "Epoch 70 - Training loss: 0.4918490486840407 - Validation loss: 0.6268885135650635\n",
      "Epoch 71 - Training loss: 0.4600551364322503 - Validation loss: 0.6145663261413574\n",
      "Epoch 72 - Training loss: 0.4651147586603959 - Validation loss: 0.6243256330490112\n",
      "Epoch 73 - Training loss: 0.45382109346489113 - Validation loss: 0.6454752683639526\n",
      "Epoch 74 - Training loss: 0.43787270598113537 - Validation loss: 0.5259780883789062\n",
      "Epoch 75 - Training loss: 0.4214697815477848 - Validation loss: 0.6250331997871399\n",
      "Epoch 76 - Training loss: 0.4396584965288639 - Validation loss: 0.5960013270378113\n",
      "Epoch 77 - Training loss: 0.4035398705552022 - Validation loss: 0.5834932327270508\n",
      "Epoch 78 - Training loss: 0.387773251781861 - Validation loss: 0.5539506077766418\n",
      "Epoch 79 - Training loss: 0.3793695991237958 - Validation loss: 0.6149476766586304\n",
      "Epoch 80 - Training loss: 0.39671805252631503 - Validation loss: 0.5532758235931396\n",
      "Epoch 81 - Training loss: 0.3888330205033223 - Validation loss: 0.5992069244384766\n",
      "Epoch 82 - Training loss: 0.39192577948172885 - Validation loss: 0.6730372309684753\n",
      "Epoch 83 - Training loss: 0.4166739533344905 - Validation loss: 0.6415860056877136\n",
      "Epoch 84 - Training loss: 0.44447602952520054 - Validation loss: 0.5082866549491882\n",
      "Epoch 85 - Training loss: 0.3029369829843442 - Validation loss: 0.4431942403316498\n",
      "Epoch 86 - Training loss: 0.3127082673211892 - Validation loss: 0.4917652904987335\n",
      "Epoch 87 - Training loss: 0.3070077697436015 - Validation loss: 0.46710067987442017\n",
      "Epoch 88 - Training loss: 0.32746178780992824 - Validation loss: 0.47997206449508667\n",
      "Epoch 89 - Training loss: 0.31906644565363723 - Validation loss: 0.4955807626247406\n",
      "Epoch 90 - Training loss: 0.2854339946061373 - Validation loss: 0.5508183240890503\n",
      "Epoch 91 - Training loss: 0.33037434704601765 - Validation loss: 0.4212888479232788\n",
      "Epoch 92 - Training loss: 0.3118029690037171 - Validation loss: 0.6061416268348694\n",
      "Epoch 93 - Training loss: 0.42761569904784363 - Validation loss: 0.6669927835464478\n",
      "Epoch 94 - Training loss: 0.34757309406995773 - Validation loss: 0.519230306148529\n",
      "Epoch 95 - Training loss: 0.30125888778517645 - Validation loss: 0.5721829533576965\n",
      "Epoch 96 - Training loss: 0.291631576915582 - Validation loss: 0.5390113592147827\n",
      "Epoch 97 - Training loss: 0.23530899515996376 - Validation loss: 0.4719542860984802\n",
      "Epoch 98 - Training loss: 0.21466885569194952 - Validation loss: 0.4467037320137024\n",
      "Epoch 99 - Training loss: 0.27494999673217535 - Validation loss: 0.4429680407047272\n",
      "Epoch 100 - Training loss: 0.26319125822434825 - Validation loss: 0.49452972412109375\n",
      "Finished training in  23.436696529388428  seconds\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.0001\n",
    "using_gpu = False\n",
    "testnbr = 1\n",
    "rnn.train_model(train_data, val_data, epochs, bsize, learning_rate, using_gpu, testnbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, out, y_test = rnn.test_model(test_data,using_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute and print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         hej       0.72      0.87      0.79        15\n",
      "       kaffe       0.86      0.90      0.88        21\n",
      "       mjÃ¶lk       1.00      0.79      0.88        24\n",
      "        tack       0.75      0.80      0.77        15\n",
      "\n",
      "    accuracy                           0.84        75\n",
      "   macro avg       0.83      0.84      0.83        75\n",
      "weighted avg       0.86      0.84      0.84        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = torch.argmax(probs,dim=1).numpy()\n",
    "truth = np.array([y.numpy() for y in y_test])\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(truth,preds,target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d14b8d03698bf490b6ff48dbb8e04bb775f0bd56b42be1a84e036e4447c0d02"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tecken': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
